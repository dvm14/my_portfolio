<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-label Classification of Radiation Type - Diya Mirji</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;500;600;700&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Open Sans', sans-serif;
            background: #F3F9D2;
            min-height: 100vh;
            color: #2d2d2d;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 60px 40px;
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            color: #3D0B37;
            text-decoration: none;
            font-size: 16px;
            margin-bottom: 40px;
            transition: all 0.3s ease;
        }

        .back-link:hover {
            transform: translateX(-5px);
            color: #63264A;
        }

        h1 {
            font-size: 48px;
            font-weight: 600;
            color: #3D0B37;
            margin-bottom: 20px;
            line-height: 1.2;
        }

        .project-meta {
            display: flex;
            gap: 20px;
            margin-bottom: 40px;
            flex-wrap: wrap;
        }

        .meta-item {
            background: rgba(255, 255, 255, 0.7);
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            color: #4B4E6D;
        }

        .content-section {
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 2px solid rgba(128, 177, 146, 0.3);
        }

        .content-section:last-child {
            border-bottom: none;
        }

        h2 {
            font-size: 32px;
            font-weight: 600;
            color: #3D0B37;
            margin-bottom: 20px;
        }

        h3 {
            font-size: 24px;
            font-weight: 600;
            color: #4B4E6D;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        p {
            line-height: 1.8;
            color: #4B4E6D;
            margin-bottom: 15px;
            font-size: 16px;
        }

        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }

        .tech-item {
            background: linear-gradient(135deg, #80B192, #6A8D92);
            color: white;
            padding: 15px 20px;
            border-radius: 8px;
            text-align: center;
            font-weight: 500;
        }

        .concept-list {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 20px;
        }

        .concept-tag {
            background: rgba(128, 177, 146, 0.2);
            color: #3D0B37;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            border: 2px solid #80B192;
        }

        .image-placeholder {
            width: 100%;
            height: 300px;
            background: linear-gradient(135deg, #6A8D92, #80B192);
            border-radius: 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 18px;
            margin: 25px 0;
        }

        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }

        .result-card {
            background: rgba(128, 177, 146, 0.1);
            padding: 20px;
            border-radius: 12px;
            border-left: 4px solid #80B192;
        }

        .result-card h4 {
            color: #3D0B37;
            font-size: 18px;
            margin-bottom: 10px;
        }

        .result-value {
            font-size: 32px;
            font-weight: 700;
            color: #4B4E6D;
            margin-bottom: 5px;
        }

        .github-link {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            background: #3D0B37;
            color: white;
            padding: 15px 30px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 600;
            transition: all 0.3s ease;
            margin-top: 20px;
            margin-right: 15px;
        }

        .github-link:hover {
            background: #63264A;
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(61, 11, 55, 0.3);
        }

        ul {
            margin-left: 20px;
            margin-top: 15px;
        }

        li {
            line-height: 1.8;
            color: #4B4E6D;
            margin-bottom: 10px;
        }

        strong {
            color: #3D0B37;
        }

        @media (max-width: 768px) {
            .container {
                padding: 40px 20px;
            }

            h1 {
                font-size: 32px;
            }

            h2 {
                font-size: 24px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link">‚Üê Back to Portfolio</a>
        
        <h1>Multi-label Classification of Radiation Type for Astronauts</h1>
        
        <div class="project-meta">
            <div class="meta-item">üìÖ June 2023</div>
            <div class="meta-item">üë• Team Project - Supervise Me</div>
            <div class="meta-item">üöÄ NASA GeneLab Collaboration</div>
        </div>

        <div class="content-section">
            <h2>Key Concepts</h2>
            <div class="concept-list">
                <div class="concept-tag">Convolutional Neural Networks (CNNs)</div>
                <div class="concept-tag">Multi-label Classification</div>
                <div class="concept-tag">Image Augmentation</div>
                <div class="concept-tag">Transfer Learning</div>
                <div class="concept-tag">Hyperparameter Tuning</div>
                <div class="concept-tag">Data Preprocessing</div>
                <div class="concept-tag">LeNet-5 Architecture</div>
                <div class="concept-tag">Model Evaluation Metrics</div>
                <div class="concept-tag">Cloud Data Processing</div>
            </div>
        </div>

        <div class="content-section">
            <h2>Overview</h2>
            <p>Astronauts experience exposure to ionizing radiation or "galactic cosmic rays" which have been linked to health hazards such as DNA damage, central nervous system effects, and immune system effects. To contribute to solving this problem, our team built machine learning models capable of analyzing patterns in mice cell images with DNA damage and classifying them by particle type (Iron vs X-ray) or both particle type and radiation dosage.</p>
            
            <div class="image-placeholder">üî¨ Fluorescent Cell Images with Radiation-Induced Foci</div>
            
            <p>Using the <strong>BPS Mice Microscopy Dataset</strong> from NASA GeneLab (77,177 images), we developed deep learning models to identify radiation types and exposure levels through visual pattern recognition of DNA damage markers. The damage appears as fluorescent foci creating linear patterns as tracks, which our ML models learned to classify.</p>
        </div>

        <div class="content-section">
            <h2>The Challenge</h2>
            <p>When astronauts experience spaceflight, ionizing radiation from galactic cosmic rays creates significant DNA damage. These rays contain approximately 87% protons, 12% Helium, and 1% high mass-energy particles through Iron. To study these effects without putting humans at risk, NASA created simulations using mice (genetically and physiologically similar to humans) at their Space Radiation Laboratory in Brookhaven National Lab.</p>
            
            <p>The challenge was to develop a system that could:</p>
            <ul>
                <li>Accurately classify radiation particle types (Iron vs X-ray) from cell images</li>
                <li>Perform multi-label classification to identify both particle type and dosage levels</li>
                <li>Process fluorescent microscopy images showing DNA damage patterns</li>
                <li>Handle images captured at different time intervals (4, 24, and 48 hours post-exposure)</li>
            </ul>

            <div class="image-placeholder">üß¨ Heavy-Ion Tracks' Damage on DNA Strands</div>
        </div>

        <div class="content-section">
            <h2>Methodology</h2>
            
            <h3>Design Sprint</h3>
            <p>The design and ideation phase was conducted as a class cohort, where we collaborated to overview the challenge, identify the problem, understand the data, and use sprint techniques to gather information and decide on project direction.</p>
            
            <p>We mapped out the process from start to finish, creating a well-defined plan for developing our model. Our class consulted with domain experts from NASA to achieve a better understanding of the problem space, identifying which aspects were technically or practically important.</p>
            
            <div class="image-placeholder">üìã Project Pipeline Created by Class</div>
            
            <p>We compiled questions and concerns about accomplishing certain tasks, which helped identify key obstacles. Each student designed a proposed end-to-end solution, reviewed by peers who identified the best parts and addressed ambiguities. Using all gathered information, we created safe, stretch, and bold goals for our project.</p>
            
            <div class="image-placeholder">üí° Brainstorming Team Goals</div>
            
            <h3>Data Preprocessing</h3>
            <p>We performed several transformations on the cell images to reduce noise and help the model focus on the linear tracks of DNA damage foci:</p>
            <ul>
                <li><strong>Normalization</strong> of image array values</li>
                <li><strong>Resizing</strong> to standardize dimensions</li>
                <li><strong>Augmentations:</strong> vertical/horizontal flipping, rotation, random cropping</li>
                <li><strong>Tensorification</strong> for PyTorch compatibility</li>
            </ul>

            <div class="image-placeholder">üîÑ Examples of Image Transformations</div>

            <h3>Model Architecture</h3>
            <p>We chose <strong>Convolutional Neural Networks (CNNs)</strong> as our framework due to their strength in processing visual data. CNNs effectively capture spatial relationships in images through their convolutional and pooling layers while significantly reducing dimensionality.</p>

            <p><strong>Baseline Model:</strong> We started with MLPClassifier from sklearn (a fully connected neural network) to establish a baseline performance benchmark.</p>

            <p><strong>Deep Learning Model:</strong> We implemented <strong>LeNet-5</strong>, a convolutional neural network with:</p>
            <ul>
                <li>Two convolutional layers with pooling</li>
                <li>ReLU activation function</li>
                <li>Adam optimizer</li>
                <li>Optimized hyperparameters through Weights & Biases sweeps</li>
            </ul>

            <p>For <strong>single-label classification</strong> (particle type only):</p>
            <ul>
                <li>Batch size: 64</li>
                <li>Epochs: 10</li>
                <li>Learning rate: 3e-4</li>
            </ul>

            <p>For <strong>multi-label classification</strong> (particle type + dosage):</p>
            <ul>
                <li>Batch size: 128</li>
                <li>Epochs: 10</li>
                <li>Learning rate: 1.5e-4</li>
            </ul>

            <h3>Performance Metrics</h3>
            <p>We evaluated our models using multiple metrics appropriate for both single and multi-label classification:</p>
            <ul>
                <li><strong>Training Loss</strong> and <strong>Validation Loss</strong></li>
                <li><strong>Validation Accuracy</strong> for exact label matching</li>
                <li><strong>Jaccard Similarity</strong> for multi-label partial matching</li>
                <li><strong>Hamming Loss</strong> for multi-label classification evaluation</li>
            </ul>
        </div>

        <div class="content-section">
            <h2>Results</h2>
            
            <div class="image-placeholder">üìä Model Performance Comparison Charts</div>
            
            <p>Our LeNet-5 architecture significantly outperformed the baseline MLP classifier across both single-label and multi-label classification tasks:</p>

            <h3>Single-Label Classification (Particle Type)</h3>
            <div class="results-grid">
                <div class="result-card">
                    <h4>LeNet-5 Validation Accuracy</h4>
                    <div class="result-value">Higher ‚úì</div>
                    <p>Superior performance vs. baseline</p>
                </div>
                <div class="result-card">
                    <h4>Validation Loss</h4>
                    <div class="result-value">Lower ‚úì</div>
                    <p>Better generalization</p>
                </div>
            </div>

            <h3>Multi-Label Classification (Particle Type + Dosage)</h3>
            <div class="results-grid">
                <div class="result-card">
                    <h4>Validation Accuracy</h4>
                    <div class="result-value">Improved</div>
                    <p>Over baseline MLP</p>
                </div>
                <div class="result-card">
                    <h4>Training Loss</h4>
                    <div class="result-value">Better</div>
                    <p>More effective learning</p>
                </div>
                <div class="result-card">
                    <h4>Jaccard Similarity</h4>
                    <div class="result-value">Higher</div>
                    <p>Better partial matching</p>
                </div>
                <div class="result-card">
                    <h4>Hamming Loss</h4>
                    <div class="result-value">Lower</div>
                    <p>Fewer label errors</p>
                </div>
            </div>

            <div class="image-placeholder">üìà Sweep Results: Hyperparameter Optimization</div>

            <p>The enhanced performance of our LeNet architecture can be attributed to:</p>
            <ul>
                <li>CNN's inherent suitability for image classification tasks</li>
                <li>Effective feature extraction through convolutional layers</li>
                <li>Reduced overfitting compared to fully connected networks</li>
                <li>Optimized hyperparameters through systematic sweeps</li>
            </ul>
        </div>

        <div class="content-section">
            <h2>What I Learned</h2>
            <p>This project provided invaluable experience in applying machine learning to real-world scientific problems:</p>

            <ul>
                <li><strong>Domain Expertise Integration:</strong> Learned to collaborate with NASA scientists and domain experts to understand the biological and physical context of the problem, making our ML solutions more meaningful and applicable.</li>
                
                <li><strong>CNN Architecture Design:</strong> Gained hands-on experience with convolutional neural networks and understood why they significantly outperform fully connected networks for image classification tasks.</li>
                
                <li><strong>Multi-label Classification:</strong> Discovered the nuances of multi-label problems and learned to use appropriate metrics (Jaccard similarity, Hamming loss) beyond simple accuracy scores.</li>
                
                <li><strong>Hyperparameter Optimization:</strong> Developed systematic approaches to tuning models using Weights & Biases sweeps, understanding the impact of batch size, learning rate, and epochs on model performance.</li>
                
                <li><strong>Cloud Data Management:</strong> Learned to work with large datasets stored in AWS S3 buckets and efficiently retrieve and process data using boto3 API.</li>
                
                <li><strong>Image Preprocessing Importance:</strong> Understood how data augmentation and preprocessing techniques can significantly improve model robustness and reduce noise in visual data.</li>
                
                <li><strong>Reproducibility Best Practices:</strong> Implemented version control, automated testing, and comprehensive documentation to ensure research reproducibility.</li>
                
                <li><strong>Real-World Impact:</strong> Experienced the rewarding challenge of building ML systems that could contribute to astronaut safety and space exploration research.</li>
            </ul>
        </div>

        <div class="content-section">
            <h2>Future Improvements</h2>
            <ul>
                <li>Implement more complex CNN architectures (AlexNet, VGGNet, ResNet)</li>
                <li>Add more convolutional layers with varying kernel sizes</li>
                <li>Explore transfer learning with pre-trained models</li>
                <li>Develop an API for real-time radiation type prediction from new cell images</li>
                <li>Create a preprocessing pipeline for user-submitted images to expand the dataset</li>
            </ul>
        </div>

        <div class="content-section">
            <h2>Technologies & Tools</h2>
            <div class="tech-grid">
                <div class="tech-item">Python</div>
                <div class="tech-item">PyTorch</div>
                <div class="tech-item">PyTorch Lightning</div>
                <div class="tech-item">AWS S3 & boto3</div>
                <div class="tech-item">Weights & Biases</div>
                <div class="tech-item">scikit-learn</div>
                <div class="tech-item">NumPy</div>
                <div class="tech-item">GitHub Actions</div>
            </div>
        </div>

        <div class="content-section">
            <h2>Links & Resources</h2>
            <a href="https://github.com/UC-Irvine-CS175/final-project-supervise-me" class="github-link" target="_blank">
                <span>View on GitHub</span>
                <span>‚Üí</span>
            </a>
            <a href="#" class="github-link" target="_blank">
                <span>All Project Documents</span>
                <span>‚Üí</span>
            </a>
            
            <p style="margin-top: 30px;"><strong>Additional Resources:</strong></p>
            <ul>
                <li><a href="https://wandb.ai/supervise-me/SAP-lnet-from-scratch/sweeps/xdvnnuuc?workspace=user-dahoang1" target="_blank" style="color: #4B4E6D;">Single-Label Model Sweep (Weights & Biases)</a></li>
                <li><a href="https://wandb.ai/supervise-me/SAP-lnet-from-scratch/sweeps/qv6zpwd3?workspace=user-dahoang1" target="_blank" style="color: #4B4E6D;">Multi-Label Model Sweep (Weights & Biases)</a></li>
                <li><a href="https://registry.opendata.aws/bps_microscopy" target="_blank" style="color: #4B4E6D;">BPS Microscopy Dataset (AWS)</a></li>
            </ul>

            <p style="margin-top: 30px; font-style: italic; color: #63264A;"><strong>Team Members:</strong> Thien Vu, Jake Leue, Darren Hoang, Diya Mirji, Nadia Ahmed (Team Mentor)</p>
            <p style="font-style: italic; color: #63264A;"><strong>Collaborators:</strong> Dr. Lauren Sanders (NASA GeneLab), Dr. Sylvain Costes (NASA)</p>
        </div>
    </div>
</body>
</html>